name: pyqir-publish-$(BuildId)

# This pipeline is used to build and publish the PyQIR package
# It uses a Microsoft ADO template for additional security checks.

# Run on merges to main to ensure that the latest code
# is always able to be published.
trigger:
  branches:
    include:
    - main

# Run the pipeline every day at 6:00 AM to ensure
# codeql and other governance checks are up-to-date.
schedules:
- cron: "0 5 * * *"
  displayName: 'Build for Component Governance'
  branches:
    include:
    - main
  always: true


variables:
  CARGO_TERM_COLOR: always
  RUST_TOOLCHAIN_VERSION: "1.86"
  PYTHON_VERSION: "3.11"
  LLVM_VERSION: 14
  # use this to invalidate the cache. Any time we update the OS
  # image, we should increment this to ensure that the cache is rebuilt.
  CACHE_COUNTER: 6

resources:
  repositories:
  - repository: 1ESPipelineTemplates
    type: git
    name: 1ESPipelineTemplates/1ESPipelineTemplates
    ref: refs/tags/release

parameters:
- name: matrix
  type: object
  default:
  - name: linux_x86_64
    poolName: 'Azure-Pipelines-DevTools-EO'
    imageName: 'ubuntu-22.04'
    os: linux
    arch: x86_64
  - name: linux_aarch64
    poolName: 'Azure-Pipelines-DevTools-ARM64-EO'
    imageName: 'mariner-2.0-arm64'
    os: linux
    arch: aarch64
  - name: mac_x86_64
    poolName: 'Azure Pipelines'
    imageName: 'macOS-latest'
    os: macOS
    arch: x86_64
  - name: mac_aarch64
    poolName: 'AcesShared'
    os: macOS
    arch: aarch64
  - name: windows_x86_64
    poolName: 'Azure-Pipelines-DevTools-EO'
    imageName: 'windows-2022'
    os: windows
    arch: x86_64
  - name: windows_aarch64
    poolName: 'Azure-Pipelines-DevTools-ARM64-EO'
    imageName: 'Windows-2022-ARM64'
    os: windows
    arch: aarch64

extends:
  template: v1/1ES.Official.PipelineTemplate.yml@1ESPipelineTemplates
  parameters:
    sdl:
      autobaseline:
        enableForGitHub: true
      sourceAnalysisPool:
        name: 'Azure-Pipelines-DevTools-EO'
        image: windows-2022
        os: windows
    stages:
    - stage: build
      displayName: Build
      jobs:
      - ${{ each target in parameters.matrix }}:
        - job: ${{ target.name }}
          pool:
            name: ${{ target.poolName }}
            image: ${{ target.imageName }}
            os: ${{ target.os }}
            ${{ if eq(target.arch, 'aarch64') }}:
              hostArchitecture: Arm64
            ${{ if eq(target.poolName, 'AcesShared') }}:
              demands:
              - ImageOverride -equals ACES_VM_SharedPool_Sequoia
          variables:
            arch: ${{ target.arch }}
            ${{ if eq(target.os, 'windows') }}:
                LLVM_DIRECTORY: $(Build.SourcesDirectory)\target\llvm
            ${{ else }}:
                LLVM_DIRECTORY: $(Build.SourcesDirectory)/target/llvm
          timeoutInMinutes: 300
          templateContext:
            outputs:
            - output: pipelineArtifact
              displayName: 'Upload Python Artifacts for ${{ target.os }} ${{ target.arch }}'
              condition: succeeded()
              targetPath: $(System.DefaultWorkingDirectory)/target/wheels
              artifactName: wheel.${{ target.os }}.${{ target.arch }}
          steps:
          - task: RustInstaller@1
            inputs:
              rustVersion: ms-$(RUST_TOOLCHAIN_VERSION)
              ${{ if target.additionalRustTargets }}:
                additionalTargets: ${{ target.additionalRustTargets }}
              cratesIoFeedOverride: $(cratesIoFeedOverride)
              toolchainFeed: $(toolchainFeed)
            displayName: Install Rust toolchain

          - script: |
              rustc --version
              rustc --print target-list
            displayName: View rust target info
            condition: succeeded()

          - task: UsePythonVersion@0
            inputs:
              versionSpec: $(PYTHON_VERSION)
              ${{ if eq(target.arch, 'aarch64') }}:
                architecture: "arm64"
            condition: not(and(eq(variables['Agent.OS'], 'Linux'), eq(variables['arch'], 'aarch64')))
          # Build or restore LLVM
          - pwsh: New-Item -ItemType Directory -Path $(LLVM_DIRECTORY) -Force
            displayName: Ensure cache dir exists
            condition: succeeded()

          - task: Cache@2
            inputs:
              key: '"llvm-$(LLVM_VERSION)-${{ target.os }}-${{ target.arch }}-$(CACHE_COUNTER)"'
              restoreKeys: '"llvm-$(LLVM_VERSION)-${{ target.os }}-${{ target.arch }}-$(CACHE_COUNTER)"'
              path: $(LLVM_DIRECTORY)
              cacheHitVar: CACHE_RESTORED
            displayName: Cache LLVM
            condition: succeeded()

          - pwsh: |
              git config --global core.longpaths true

              $directory = Resolve-Path $(LLVM_DIRECTORY)

              # Tell the build where LLVM is installed
              # This is usually automatic, but we are using a different directory than the default
              Write-Host "##vso[task.setvariable variable=QIRLIB_LLVM_EXTERNAL_DIR;]$directory"

              # Tell the build what llvm feature version to use
              $feature = "llvm$(LLVM_VERSION)-0"
              $variableName = "LLVM_SYS_$(LLVM_VERSION)0_PREFIX"
              Write-Host "##vso[task.setvariable variable=PYQIR_LLVM_FEATURE_VERSION;]$feature"

              # Tell LLVM-SYS where LLVM is installed
              Write-Host "##vso[task.setvariable variable=$variableName;]$directory"

              # Tell the qirlib build where to cache the LLVM directory and to not download LLVM
              Write-Host "##vso[task.setvariable variable=QIRLIB_CACHE_DIR;]$directory"
              Write-Host "##vso[task.setvariable variable=QIRLIB_DOWNLOAD_LLVM;]false"
            displayName: Configure Environment
            condition: succeeded()

          # prerequisites for building
          - script: |
              sudo apt-get install -y ninja-build
            displayName: Install build dependencies
            condition: and(succeeded(), eq(variables['Agent.OS'], 'Linux'), eq(variables['arch'], 'x86_64'), ne(variables.CACHE_RESTORED, 'true'))

          - script: |
              sudo tdnf install ninja-build cmake -y
            displayName: Install c++ build tools on mariner
            condition: and(succeeded(), eq(variables['Agent.OS'], 'Linux'), eq(variables['arch'], 'aarch64'), ne(variables.CACHE_RESTORED, 'true'))

          - script: |
              brew install cmake ninja ccache
            displayName: Install build dependencies
            condition: and(succeeded(), eq(variables['Agent.OS'], 'Darwin'), ne(variables.CACHE_RESTORED, 'true'))

          # on windows x86_64 we can install ninja via chocolatey. The cmake is pre-installed on the image.
          - script: |
              choco install --accept-license -y ninja
            displayName: Install build dependencies
            condition: and(succeeded(), eq(variables['Agent.OS'], 'Windows_NT'), eq(variables['arch'], 'x86_64'), ne(variables.CACHE_RESTORED, 'true'))

          # ninja is not available on windows aarch64 images, so we install it from its github releases
          - pwsh: ./build.ps1 -t install-ninja
            displayName: Install build dependencies
            condition: and(succeeded(), eq(variables['Agent.OS'], 'Windows_NT'), eq(variables['arch'], 'aarch64'), ne(variables.CACHE_RESTORED, 'true'))

          # all platforms: verify ninja is installed
          - pwsh: ./build.ps1 -t verify-ninja
            displayName: Verify ninja build dependencie
            condition: and(succeeded(), ne(variables.CACHE_RESTORED, 'true'))

          # on mariner we need to install:
          # ld with binutils
          # crt1.o and others with glibc-devel
          # std lib headers with kernel-headers
          # ninja and cmake for building LLVM
          - script: |
              sudo tdnf install binutils glibc-devel kernel-headers -y
            displayName: Install c++ build tools on mariner
            condition: and(succeeded(), eq(variables['Agent.OS'], 'Linux'), eq(variables['arch'], 'aarch64'))

          # build and install LLVM

          - pwsh: ./build.ps1 -t install-llvm-from-source
            displayName: Build and Install LLVM
            condition: and(succeeded(), not(canceled()), not(failed()), ne(variables.CACHE_RESTORED, 'true'), ne(variables['Agent.OS'], 'Darwin'))

          - script: |
              echo "env ARCHFLAGS='$(ARCHFLAGS)' ./build.ps1 -t install-llvm-from-source"
              env ARCHFLAGS="$(ARCHFLAGS)" ./build.ps1 -t install-llvm-from-source
            displayName: Build and Install LLVM
            condition: and(succeeded(), not(canceled()), not(failed()), ne(variables.CACHE_RESTORED, 'true'), eq(variables['Agent.OS'], 'Darwin'))

          # Build PyQIR
          - pwsh: ./build.ps1 -t default
            displayName: Build
            condition: and(succeeded(), ne(variables['Agent.OS'], 'Darwin'))

          - pwsh: env ARCHFLAGS="$(ARCHFLAGS)" ./build.ps1 -t build
            displayName: Build
            condition: and(succeeded(), eq(variables['Agent.OS'], 'Darwin'))

          - pwsh: ./build.ps1 -t test
            displayName: Build
            condition: succeeded()

          - script: |
              dir target\wheels\*
            displayName: List Py Packages on Win
            condition: and(succeeded(), eq(variables['Agent.OS'], 'Windows_NT'))

          - script: |
              ls target/wheels/*
            displayName: List Py Packages on non-Win
            condition: and(succeeded(), ne(variables['Agent.OS'], 'Windows_NT'))

    - stage: approval
      displayName: Approval
      dependsOn: build
      condition: and(succeeded(), eq(variables['Build.Reason'], 'Manual'))
      jobs:
      - job: "Approval"
        pool: server
        timeoutInMinutes: 1440 # job times out in 1 day
        steps:
        - task: ManualValidation@0
          timeoutInMinutes: 1440 # task times out in 1 day
          inputs:
            notifyUsers: ''
            instructions: 'Please verify artifacts and approve the release'
            onTimeout: 'reject'

    - stage: release
      displayName: Release
      dependsOn: approval
      condition: and(succeeded(), eq(variables['Build.Reason'], 'Manual'))
      jobs:
      # We will get a warning about extra files in the sbom validation saying it failed.
      # This is expected as we have the wheels being downloaded to the same directory.
      # So each successive wheel will have the previous wheel in the directory and each
      # will be flagged as an extra file. See:
      # http://aka.ms/drop-validation-failure-additional-files
      - job: "Publish_Python_Packages"
        pool:
          name: 'Azure-Pipelines-DevTools-EO'
          image: 'ubuntu-latest'
          os: linux
        templateContext:
          type: releaseJob
          isProduction: true
          inputs:
          - ${{ each target in parameters.matrix }}:
            - input: pipelineArtifact
              artifactName: wheel.${{ target.os }}.${{ target.arch }}
              targetPath: $(System.DefaultWorkingDirectory)/target/wheels
        steps:
        - script: |
            ls $(System.DefaultWorkingDirectory)/target/wheels
          displayName: Display Py Artifacts in Publishing Dir

        - task: EsrpRelease@9
          condition: succeeded()
          displayName: Publish Py Packages
          inputs:
            connectedservicename: 'PME ESRP Azure Connection'
            usemanagedidentity: true
            keyvaultname: 'quantum-esrp-kv'
            signcertname: ESRPCert
            clientid: '832c049d-cd07-4c1c-bfa5-c07250d190cb'
            contenttype: PyPi
            domaintenantid: '975f013f-7f24-47e8-a7d3-abc4752bf346'
            folderlocation: '$(System.DefaultWorkingDirectory)/target/wheels'
            waitforreleasecompletion: true
            owners: 'billti@microsoft.com'
            approvers: 'billti@microsoft.com'
            mainpublisher: ESRPRELPACMAN
